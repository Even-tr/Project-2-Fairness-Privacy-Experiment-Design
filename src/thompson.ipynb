{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from covid.simulator import Population\n",
    "from covid.auxilliary import symptom_names\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from covid.policy import Policy\n",
    "from scipy.stats import beta, bernoulli, uniform"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class ThompsonSampling:\n",
    "    def __init__(self, nvacc):\n",
    "        # Priors for the beta-bernoulli model\n",
    "        self.a = np.ones(nvacc) # uniform prior\n",
    "        self.b = np.ones(nvacc) # uniform prior\n",
    "        self.nvacc = nvacc\n",
    "\n",
    "    def update(self, action, outcome):\n",
    "        self.a[action] += outcome\n",
    "        self.b[action] += 1 - outcome\n",
    "\n",
    "    def get_params(self):\n",
    "        # Returns the parameters of all the beta distrobutions.\n",
    "        return self.a, self.b\n",
    "\n",
    "    def get_prob(self):\n",
    "        return beta.rvs(self.a, self.b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "class Naive(Policy):\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def get_action(self):\n",
    "        ret_val = np.argmax(self.model.get_prob)\n",
    "        print(ret_val)\n",
    "        return ret_val\n",
    "\n",
    "    def observe(self, action, outcome):\n",
    "        self.model.update(action, outcome)\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "thetas = np.array([0.6, 0.55, 0.5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "n_genes = 128\n",
    "n_vaccines = 3\n",
    "n_treatment = 4\n",
    "#population = Population(n_genes, n_vaccines, n_treatment)\n",
    "N = 100000\n",
    "#X = population.generate(N)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "policy = Naive(n_actions=3, action_set=[0, 1, 2])\n",
    "model = ThompsonSampling(nvacc=3)\n",
    "policy.set_model(model)\n",
    "\n",
    "action_arr = np.zeros(N)\n",
    "symptom_arr = np.zeros(N)\n",
    "\n",
    "for i in range(10):\n",
    "    action = policy.get_action()\n",
    "    reward = bernoulli.rvs(thetas[action])\n",
    "    policy.observe(action, reward)\n",
    "    #print(action, reward)\n",
    "    #policy.observe(action, reward)\n",
    "    #print(action)\n",
    "\n",
    "alphas, betas = policy.model.get_params()\n",
    "for i, j in zip(alphas, betas):\n",
    "    print(i,j)\n",
    "#print(alphas[0]/(alphas[0] + betas[0]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initialising policy with  3 actions\n",
      "A = { [0, 1, 2] }\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "9.0 3.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "51600fd56a40f342b1560ff0a624a3ca9526e59d8d11c813ce0c927098550455"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}