

\documentclass[a4paper, 12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{color}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{tikz} 
\usepackage{verbatim}
%\usepackage[a4paper,
%            bindingoffset=0.2in,
%            left=0.5in,
%            right=1in,
%            top=1in,
%            bottom=0.5in,
%            footskip=.25in]{geometry}

\title{IN-STK5000 Project 2 - Project 5}
\author{Anders Bredesen Hatlelid \\
        Jacob Nicolai Arthur Sjødin \\
        \and
        Even Tronstad \\
        Torgeir Ladstein Waagbø 
}

\date{\today}
\begin{document}
\maketitle

\noindent \textcolor{red}{There is a lot of text, but I have stated to questions in red and tried to be specific}
\subsubsection*{Two separate privacy questions}
I interpret the privacy questions as two separate cases: One where you are asked to protect the identities of the people in the training set (just the storage of the data, without taking the experiment into account). It is this question we shall address when answering:
\begin{itemize}
    \item \textcolor{blue}{Does the existence of this database raise any privacy concerns?}
    \item \textcolor{blue}{Explain how you would protect the data of the people in the training set.}
\end{itemize}
This we shall only describe, and not implement. 
\\
\\
The second questions is how to can protect the people in the training set from being identified based on the published data in the report. This we shall address in the questions: 
\begin{itemize}
    \item \textcolor{blue}{If the database was secret, but your analysis public, how would that affect privacy?}
    \item \textcolor{blue}{In particular, given that your policy and model are obtained from some 'training' data set, how would you guarantee that release or use, of the policy and model does not leak private information about the individual?}
    \item \textcolor{blue}{Explain how you would protect the data of the people that obtained treatment?}
    \item \textcolor{blue}{How would you then ensure that the private information of the treated individual is not leaked?}
\end{itemize}
\noindent \textcolor{red}{Q1: Have I somewhat understood the questions we shall answer?}

\subsubsection*{Implementation of privacy}
Based on the two type of questions, I interpret the point 
\begin{itemize}
    \item \textcolor{blue}{Implement a private decision making mechanism for (b)}
\end{itemize}
as being asked to implement a privacy mechanism NOT to hide the individual in the database from being identifiable, but to make sure the individuals can not be identified based on our analysis, assuming the database to be secret in the first place.
\\
\noindent \textcolor{red}{Q2: Is this correct?}


\subsubsection*{Privacy when using bandits}
We have chosen to stick with the bandit model using Thompson sampling. 
We have three bandits, one for each vaccine.
Thus we disregard the features, but we will address this limitation. (We considered changing the bandits to model the probability of death given comorbidities, but we stuck to vaccines as we find this model simple and we understand it.)

Here is my thoughts on how to use privacy using bandits: We add Laplace noise when updating the parameters for the bandits. 
If we use a Local privacy model we would get
\begin{align*}
    \theta_i \sim \text{Beta}(a_i, b_i) \quad i=1,2, 3
\end{align*}
We choose the argmax and observe the reward $r_i$ (whether or not we get a critical symptom). When we update the parameters we add the noise $\omega \sim \text{Laplace}$. Or we could use a centralised privacy model we we batch the updates and add noise for each batch. 

\noindent \textcolor{red}{Q3: Is this a sufficient privacy mechanism? Or do we need to we also need to add noise to the observed symptom (response) when 'pulling' a bandit?}
\noindent \textcolor{red}{Q4: And if we need to add noise to the symptom, can we use randomized response?}

\subsubsection*{The exponential mechanism}
We have considered trying the exponential mechanism, but we find it hard to grasp. 
\\
In Dirk's lecture he had an example where he used the exponential mechanism to change the frequencies of the values in a 'education'-column. 
However, the definition in the book (3.4.6)
\begin{align*}
    \pi(a|x) := \frac{e^{\epsilon U(q,a,x) / \mathbb{L}(U(q))}}{e^{\sum_{a'}\epsilon U(q,a',x) / \mathbb{L}(U(q))}}
\end{align*}
defines the probability of a policy.

When answering the question: 
\begin{itemize}
    \item \textcolor{blue}{Estimate the amount of loss in utility as you change the privacy guarantee}
\end{itemize}
my initial though was to add Laplace noise to the beta parameter updates, and then estimate the accumulated utility. This would probably be sub optimal and result in loss in utility.

\noindent \textcolor{red}{Q5: If you think the exponential mechanism is a good idea, could you give us a hint on how we use it in the bandit setting?}
\end{document}