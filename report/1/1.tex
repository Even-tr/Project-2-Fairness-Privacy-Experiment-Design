\section*{Privacy analysis}

\subsection*{Database privacy concerns}
The existence of this database raises some privacy concerns. With such a high number of genes (128), we get a total of $2^{128}$ possible combinations of genes. This would make it very unlikely for two people to have the same genes, and so it would be easy so single out people from this database if we know their genes. 
    
Important here is the discussion of $k$-anonymity. This is when any row in the database is indistinguishable from $k-1$ other rows with respect to a given set of identifiers. The identifiers are defined by the analyst. For a public database, the identifiers are essentially all the features. In our case, the number of options make it highly likely that the database satisfies no more than 1-anonymity. 
    
This might not be such a big problem if we consider the fact that the information of a persons genes is not common to have, but there have been a growing number of services where people exchange their genetic material for knowledge about their ancestry. These services could be hacked and then that information could get into a malicious adversary's hands. 
    
From a differential privacy point of view, an adversary is assumed to have all available side information anyway. From this point of view, the database raises huge concerns. 
    
One way of dealing with this is to reduce the information obtainable in the database by cutting down on the number of genes. If we suspect that only a few number of genes matter, then we can exclude the rest from the database. We can bucket together the income groups if we want to anonymize further. 

\subsection*{Private data analysis}
The model we are using in this project is that the database is secret, but that the analysis is public. This simplifies privacy concerns, as we do not have to guarantee $k$-anonymity. However, analysis of the data can still reveal enough information to identify individuals. A way of evading this issue is to use differential privacy mechanisms to add some noise to the statistics. 

A stochastic algorithm $\pi : X \rightarrow A$ where the database $X$ is endowed with a neighbourhood relation $N$ is said to be $\epsilon$-differentially private if:
\begin{align*}
    \frac{\pi(a|x)}{\pi(a|x')} \leq e^\epsilon
\end{align*}
in all cases where $xNx'$. We typically define the neighbourhood relation so that two databases are neighbouring if they can be reached from each other by adding or removing a row. 
    
In our case we use the Laplace mechanism to update a Beta distribution with a Bernoulli prior in a Bayesian framework. We work in the \emph{centralised model}, which is the standard model in differential privacy. Here we add on Laplace noise $\omega \sim \operatorname{Laplace}(\lambda)$ where $\lambda$ is a parameter which depend on what query we are giving in order for the mechanism to be $\epsilon$-differential private and the \emph{sensitivity} of the query. The sensitivity $s$ of a query is given by 

\begin{align*}
    s := \max_{xNy} |f(x) - f(y)|.
\end{align*}

For the Laplace mechanism, we have that $\lambda = s/\epsilon$. In our case, we update by counting the number of person get critical symptoms from the vaccine. This is a counting query, and thus we can use results about general counting queries, namely that their sensitivity is 1. Thus for a counting query, we get that $\lambda = 1/\epsilon$, where $\epsilon$ is a parameter between bigger than zero that determines the degree of privacy. 

In our approach we update the parameters for every person, and include Laplace noise every time. Another approach is to retrain using our original prior and accumulated data. In this case, we must consider the privacy of repeated queries to our database. With a total of $Q$ queries we get a total DP of
\begin{align*}
    \sum_{i=1}^Q\epsilon = Q\epsilon.
\end{align*}
To still achieve the same $\epsilon$-differentially privacy, we must use $\epsilon_Q = \epsilon/Q$. This is a trade off, as it would reduce the total noise at the end of our calculations, but would incure more noise at the beginning. 

\subsection*{Protecting privacy for training and outcome}
We are adaptive treatment allocation, via Thompson sampling, in this project. The training data and the data that obtains treatment is the same. The outcome of the vaccination of a single person is depended on the previous results from the vaccination. 
As the decision to vaccinate is based purely on the data from the previous people, so the decision to vaccinate is independent of the current person. Thus private information of treated is not leaked at all through our method. 

\subsection*{Exponential mechanism}
 We also employ an exponential mechanism for our utility. This gives a policy which guarantees $\epsilon$-differential privacy. We implement this to show the differences it gives. It is given by
            \begin{align*}
                \pi(a|x) = \frac{ e^{\epsilon U(q,a,x)/s}}{\sum_{a'} e^{\epsilon U(q,a',x)/s}}
            \end{align*}
where $s$ is the sensitivity of the query. 

\begin{itemize}
    \item \textcolor{blue}{Does the existence of this database raise any privacy concerns?}
        \begin{itemize}
            \item What are som particularily sentitive variables, f.ex genes?
            \item The existence of the database does indeed raise privacy concerns. 
            In particular, a persons genome might be used to identify individuals. 
            \begin{itemize}
                \item TODO: discuss different privacy mechanishms. 
                \item Be more specific?
                \item Reflect on which of the variables are most important for identification. 
            \end{itemize}
            \item Other points from brainstorming: Mention: 
        \end{itemize}
    \item \textcolor{blue}{If the database was secret, but your analysis public, how would that affect privacy?}
        \begin{itemize}
            \item This is actually the case. 
            \item Might be possible to identify individual from the summary statistics. 
            \item TODO: discuss the points Anonymity, Secrecy, side-information and Utility on page 76 in the notes. 
            \item From brainstorming: Outliers like high age? Perhaps state the most most critical features. 
            \item How I interpret the question: We assume an adverasry to have perfect knowledge and unlimited computer power. 
            We then need to ensure than he cannot identify the individual that participate in the study. 
        \end{itemize}
    \item \textcolor{blue}{(a)}
        \begin{itemize}
            \item \textcolor{blue}{Explain how you would protect the data of the people in the training set.}
                \begin{itemize}
                    \item Need a general discussion about how to make a database private.  
                \end{itemize}
            \item \textcolor{blue}{In particular, given that your policy and model are obtained from some 'training' data set, how would you guarantee that release, or use, of the policy and model does not leak private information about the individuals?}
                \begin{itemize}
                    \item Here we need to discuss how to to make the desition and model private. 
                    Not sure if the individual reponses to the vaccines need to be private. 
                \end{itemize}
        \end{itemize}
    \item \textcolor{blue}{(b)}
        \begin{itemize}
            \item \textcolor{blue}{Explain how would protect the data of the people that obtain treatment.}
                \begin{itemize}
                    \item Is this with regard to just the storage of the data in the database?
                \end{itemize}
            \item \textcolor{blue}{When you apply the policy or model to decide what treatment to give, this decision can be assumed to be publicly available.}
                \begin{itemize}
                    \item 
                \end{itemize}
            \item \textcolor{blue}{How would you then ensure that the private information of the treated individual is not leaked?}
                \begin{itemize}
                    \item Here it is the case that we need to answer how to prevent the identification of the individual based on the information that is public. 
                    The public information we release is vaccine desi
                \end{itemize}
        \end{itemize}
    \item \textcolor{blue}{(c)}
        \begin{itemize}
            \item \textcolor{blue}{Implement a private decision making mechanism for (b).}
                \begin{itemize}
                    \item 
                \end{itemize}
        \end{itemize}
    \item \textcolor{blue}{(d)}
        \begin{itemize}
            \item \textcolor{blue}{Estimate the amount of loss in utility as you change the privacy guarantee.}
                \begin{itemize}
                    \item 
                \end{itemize}
        \end{itemize}
\end{itemize}